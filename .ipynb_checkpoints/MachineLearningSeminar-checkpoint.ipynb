{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSU Machine Learning Seminar - Classification and Clustering in Python\n",
    "\n",
    "In this notebook, we will show how to put some of the theory we've been learning about into practice.\n",
    "\n",
    "This lesson is written in a \"Jupyter notebook\", which is a useful way to organize and present Python code. Each of the cells containing code could be evaluated in a command line enviroment as well---this is the real Python code.\n",
    "\n",
    "To evaluate a cell, hold Shift and press Return.\n",
    "\n",
    "To create a new cell, highlight a cell by clicking on the left of it, then type \"a\" or \"b\" for \"above\" or \"below\", respectively. You can also use the \"+\" button in the menu bar.\n",
    "\n",
    "To delete a cell, highlight it and press \"d\" twice in succession. \n",
    "\n",
    "To create a cell for text rather than code (such as this one), create a cell then switch it to \"Markdown\" mode in the dropdown above. You can also highlight the cell and type \"m\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Extremely) Basic Python\n",
    "\n",
    "Evaluate the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0^0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1^0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "1^1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat+dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'cat'+'dog'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example - Iris Dataset\n",
    "\n",
    "The `iris` dataset is a famous dataset used in examples. It contains biological measurements for samples of a few different flower species. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import this dataset through a Python package called `seaborn`. In Python programming, you use a lot of packages that people have already built. You will need to install some of them. I recommend installing a data science platform like `Anaconda`; this will automatically install a bunch of useful data science Python packages.\n",
    "\n",
    "Packages are loaded by the `import` command, and can be loaded with abbreviations to make your code clearner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns # sns is the standard abbreviation for seaborn\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset loads as something called a \"dataframe\". This is like a fancy spreadsheet with useful commands for exploring and subsetting the data.\n",
    "\n",
    "Let's take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record has four mearements or \"features\" and a label. This means that we can use our machine learning techniques to train a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains flowers from three different species, as we can see below. This means that the labels for this data naturally fall into three categories, say, $\\{0,1,2\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create a scatterplot for the features, we see that the data is pretty well-separated by label.\n",
    "\n",
    "The code is calling the `scatterplot` function from the package `seaborn`, which we abbreviated as `sns`. The input data to the `scatterplot` function is making use of the dataframe structure of the `iris` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"petal_length\", y=\"petal_width\",\n",
    "                hue=\"species\", data=iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "*Logistic regression* is a regression algorithm on data consisting of\n",
    "- A set of feature vectors $X = \\{\\vec{x}_1,\\ldots,\\vec{x}_n\\}$, each $\\vec{x}_j \\in \\mathbb{R}^d$\n",
    "- A set of labels $Y = \\{y_1,\\ldots,y_n\\}$, each $y_j \\in \\{0,1\\}$. \n",
    "The goal is to find a function $f:\\mathbb{R}^d \\rightarrow [0,1]$ such that $f(\\vec{x})$ predicts the probability that a feature vector $\\vec{x}$ should be labeled $1$. \n",
    "\n",
    "The regression problem is solve by finding weights $\\beta_1,\\ldots, \\beta_d, b$ minimizing the loss function\n",
    "$$\n",
    "L(\\beta_1,\\ldots,\\beta_d,b) = -\\frac{1}{n} \\sum_j \\big[y_j \\log(f(\\vec{x}_j)) + (1-y_j)\\log(1-f(\\vec{x}_j)) \\big],\n",
    "$$\n",
    "where $f(\\vec{x}) = S(g(\\vec{x}))$, with $g$ the linear function\n",
    "$$\n",
    "g(\\vec{x}) = \\beta_1 x^1 + \\cdots + \\beta_d x^d + b\n",
    "$$\n",
    "and $S$ the *sigmoid function* (or *logistic function*)\n",
    "$$\n",
    "S(a) = \\frac{1}{1 + \\exp(-a)}.\n",
    "$$\n",
    "This optimal collection of weights is found via gradient descent, or some variant of it.\n",
    "\n",
    "### Increasing the Number of Labels\n",
    "\n",
    "Now suppose we have data of the form\n",
    "- A set of feature vectors $X = \\{\\vec{x}_1,\\ldots,\\vec{x}_n\\}$, each $\\vec{x}_j \\in \\mathbb{R}^d$\n",
    "- A set of labels $Y = \\{y_1,\\ldots,y_n\\}$, each $y_j \\in \\{0,1,\\ldots,K\\}$, $K \\geq 1$. I.e., now there are $K+1$ labels that we would like to predict!\n",
    "\n",
    "The goal is now to find a function \n",
    "$$\n",
    "f:\\mathbb{R}^d \\rightarrow [0,1] \\times [0,1] \\cdots \\times [0,1],\n",
    "$$\n",
    "where the range has $K+1$ copies of the interval $[0,1]$. Moreover, we want the output to be a *probability vector*. That is, if $f(\\vec{x}) = \\vec{p}$, with $\\vec{p} = (p^0,p^1,\\ldots, p^K)$ satisfying\n",
    "$$\n",
    "\\sum_{j=0}^K p^j = 1.\n",
    "$$\n",
    "We could use such a function to label feature vectors via the rule\n",
    "$$\n",
    "\\mathrm{label}(\\vec{x}) = \\mathrm{argmax}_j \\{p_j\\},\n",
    "$$\n",
    "i.e., the feature recieves the label with the highest probability.\n",
    "\n",
    "### Formulating an Algorithm: SoftMax Regression\n",
    "\n",
    "The precise statement of the problem falls into a familiar paradigm: we want to fix a class of relevant predictor functions, parameterized by some weights, then minimize a loss function on those weights. This is done by a rather direct generalization of the logistic regression formulation.\n",
    "\n",
    "We now have a collection of weights for each label. Denote the weights for label $j$ by $\\beta^{(j)}_1, \\ldots, \\beta^{(j)}_d, b^{(j)}$. For each label $j$, we then get a linear function $g^{(j)}$ defined on $\\vec{x} = (x^1,\\ldots,x^d)$ by\n",
    "$$\n",
    "g^{(j)}(\\vec{x}) = \\beta^{(j)}_1 x^1 + \\cdots + \\beta^{(j)}_d x^d + b^{(j)}.\n",
    "$$\n",
    "Finally, we define \n",
    "$$\n",
    "p^j(\\vec{x}) = \\frac{\\exp(g^{(j)}(\\vec{x}))}{\\sum_{\\ell=0}^K \\exp(g^{(\\ell)}(\\vec{x}))}.\n",
    "$$\n",
    "Note that $p^j(\\vec{x}) \\in [0,1]$. We interpret this as the probability that $\\vec{x}$ has label $j$. Our prediction function is then given by\n",
    "$$\n",
    "f(\\vec{x}) = \\vec{p}(\\vec{x}) = \\left(p^0(\\vec{x}),p^1(\\vec{x}),\\ldots,p^K(\\vec{x})\\right).\n",
    "$$\n",
    "\n",
    "Let $B$ denote the collection of all $\\beta^{(j)}_\\ell$ (so $B$ contains $d\\cdot K$ weights) and let\n",
    "$$\n",
    "b = (b^{(0)},\\ldots,b^{(K)})\n",
    "$$\n",
    "denote the vector of offsets (sometimes called *biases*). We seek to optimize the following loss function\n",
    "$$\n",
    "L(B,b) = -\\frac{1}{n} \\sum_{j=1}^n \\sum_{\\ell = 0}^K \\delta_{j\\ell} \\log(p^\\ell(\\vec{x}_j)),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\delta_{j\\ell} = \\left\\{\\begin{array}{cc}\n",
    "1 & y_j = \\ell \\\\\n",
    "0 & \\mbox{otherwise.} \\end{array}\\right.\n",
    "$$\n",
    "This loss function $L$ is sometimes called the *cross entropy function*.\n",
    "\n",
    "### Theoretical Homework\n",
    "\n",
    "Check that this reduces to the standard logistic regression problem when the labels are $\\{0,1\\}$.\n",
    "\n",
    "### Solving the Regression Problem\n",
    "\n",
    "The cross entropy function is a differentiable function $L:\\mathbb{R}^{dK + K} \\rightarrow \\mathbb{R}$. It is convex and therefore has a unique minimum. However, it is not possible to solve for the minimum analytically. Luckily, one can compute the gradient $\\nabla L$ explicitly and we can therefore minimize the loss via gradient descent (or some variant thereof). We will use built-in functions from the package `scikit-learn`, rather than coding this gradient descent by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax Regression on the Iris Dataset\n",
    "\n",
    "Let's train and test a model to classify all 3 flower species in the `iris` dataset.\n",
    "\n",
    "We will use the popular Machine Learning Python package `scikit-learn`. We will import certain functions from the package, rather than the whole package as we did for `seaborn`.\n",
    "\n",
    "We'll also use the package `matplotlib.pyplot` for plotting things. This is another very popular package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our data. The explanatory variables are 'petal_length' and 'petal_width' (so there are two features, and each element of our set $X$ is a vector in $\\mathbb{R}^2$). We also need to get the set $y$ consisting of labels for all of the flower species. The `scikit-learn` package has a built-in function to take a vector of 'categorical variables' (i.e., labels) and turn it into a vector of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[['petal_length','petal_width']] #'Subsetting' the dataframe to take two of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris.species.astype(\"category\").cat.codes # Extracting labels as 0,1,2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the last function did the right thing by plotting. We use the function `plot` from the package `matplotlib.pyplot`, which we abbreviated as `plt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the label vector shows that we have correctly labelled things as 0,1,2\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the performance of logistic regression for this classification problem, we will split our data into a 'testing set' and a 'training set'. \n",
    "\n",
    "We imported a function from `scikit-learn` which does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Check the sizes of the sets\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've pulled off 112 feature vectors and their labels. This is the \"training set\". This will be used to train the model. We can then test its performance on the data which was left out. This is the \"testing set\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit the logistic regression model to our training data. Notice that we have to specify a `multi_class` option (otherwise we will get a warning). The `multinomial` option uses the cross entropy function, as described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='lbfgs', multi_class = 'multinomial', max_iter=10000) # Choose a model\n",
    "model.fit(X_train, y_train) # Train the model on our \"training set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight coefficients $\\beta^{(j)}_\\ell$ and $b^{(j)}$ are given below. There should be \n",
    "$$\n",
    "dK + K = 2\\cdot 3 + 3 = 9\n",
    "$$\n",
    "numbers in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our probability functions are defined for $j=0,1,2$ by\n",
    "$$\n",
    "p^j(\\vec{x}) = \\frac{\\exp(g^{(j)}(\\vec{x}))}{\\sum_{\\ell=0}^K g^{(\\ell)}(\\vec{x})},\n",
    "$$\n",
    "with\n",
    "\\begin{align*}\n",
    "g^{(0)}(\\vec{x}) &= -2.535 x^1 - 1.026 x^2 + 10.125 \\\\\n",
    "g^{(1)}(\\vec{x}) &= 0.10855954 x^1 -0.81556404 x^2 + 2.69393459\\\\\n",
    "g^{(2)}(\\vec{x}) &= 2.42641431 x^1 +  1.84181147 x^2 -12.81878324\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how these probability functions evaluate on some subset of our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(X_test[0:5])\n",
    "print(y_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict_proba(X_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(5):\n",
    "    print('Sum of row:', sum(model.predict_proba(X_test[0:5])[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row above gives the three probabilities that the correct label is 0, 1 or 2, respectively. If we take the max in each row, then the predicted labels agree with the true ones! Indeed, this can be done with a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model mades some correct predictions! We can plot each of the probability functions over the full data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "for j in range(3):\n",
    "    fig.add_subplot(1,3,j+1)\n",
    "    plt.plot(model.predict_proba(X)[:,j])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a pretty convincing split between the flower species. Some observations:\n",
    "- In the function on the left, everything in class 2 has essentially zero probability of being labelled as class 0\n",
    "- For the function in the middle and one the right, there is a very low probability of assigning the label 0\n",
    "- For the function in the middle (probability that the label is 1), there are several things with true label 2, but which have high values.\n",
    "\n",
    "The model still performs really well in the classification task, but not perfectly, as we see by the following scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what was predicted correctly/incorrectly, we can look at the *confusion matrix*. For a general multiclass classification problem with labels $0,1,\\ldots,K$, the confusion matrix is the $(K+1) \\times (K+1)$ matrix\n",
    "$$\n",
    "C = (C_{ij}) = \\left(\\begin{array}{cccc}\n",
    "C_{00} & C_{01} & \\cdots & C_{0K} \\\\\n",
    "C_{10} & C_{11} & \\cdots & C_{1K} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "C_{K0} & C_{K1} & \\cdots & C_{KK} \\end{array}\\right)\n",
    "$$\n",
    "with entry $C_{ij}$ giving the number of observations known to be in group $i$ and predicted to be in group $j$.\n",
    "\n",
    "This can be computed via `scikit-learn` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predicted = model.predict(X_test)\n",
    "metrics.classification_report(y_test, predicted)\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that in the test set almost everything was classified correctly, except one flower with true label 1 was predicted to have label 2. \n",
    "\n",
    "We can similarly compute the confusion matrix on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_train)\n",
    "metrics.classification_report(y_train, predicted)\n",
    "print(metrics.confusion_matrix(y_train, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Rerun the above experiments using features `sepal_length` and `sepal_width`. Before fitting the model, plot the data. Do you expect better or worse classification results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data\n",
    "\n",
    "In order to get a feel for how various ML algorithms work, it is extremely beneficial to play around with toy data. \n",
    "\n",
    "The code below generates some toy data. You can play with the various parameters in the `make_blobs` function until you get something that looks interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(centers=4, n_samples = 5000, center_box = [-4,4], random_state=7)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.title(\"Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run through the same model training/testing process as we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Check the sizes of the sets\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', multi_class = 'multinomial', max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below visualizes the performance of the logistic regression classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label = 1 # Change this parameter to change the plot.\n",
    "\n",
    "xx, yy = np.mgrid[-6:6:.01, -6:6:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = model.predict_proba(grid)[:, label].reshape(xx.shape)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                      vmin=0, vmax=1)\n",
    "ax_c = f.colorbar(contour)\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "ax.scatter(X[:,0], X[:,1], c=y)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(-6, 6), ylim=(-6, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Rerun the above experiment after playing around with the various parameters in the toy data generator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` package includes a collection of toy\n",
    "datasets, including the ever-popular MNIST database (Modified\n",
    "National Institute of Standards and Technology database), a database\n",
    "of handwritten digits. MNIST is used as a standard testing ground and benchmark when studying machine learning algorithms. \n",
    "\n",
    "The data is highly preprocessed to center the\n",
    "handwritten digits, threshold the bitmaps, etc. This particular version of MNIST is smaller than the full version that you see used as a benchmark in ML papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Bunch' is a dictionary-type object. Let's take a look at what is in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are available under `digits.images`. The labels of the digits are in `digits.target`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = digits.images\n",
    "print(type(images))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = digits.target\n",
    "target[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is an $8 \\times 8$ array of numbers between 0 and 16,\n",
    "inclusive.  Let's take a look at the first few examples.\n",
    "\n",
    "Note that image generation can be done in a `for` loop in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(2,5,j+1)\n",
    "    plt.imshow(digits.images[j], cmap='gray')\n",
    "    # imshow is a useful function. \n",
    "    # It treats an array of numbers as an image, with the number in each entry \n",
    "    # corresponding to a color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can blur the pixels a bit to get better looking pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(1,10,j+1)\n",
    "    plt.imshow(digits.images[j], cmap='gray', interpolation = 'sinc')\n",
    "    plt.axis('off')\n",
    "    # Using different display options than what we used previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat each image as a vector in $\\mathbb{R}^{8 \\times 8} \\approx \\mathbb{R}^{64}$. Conveniently, `digits.data` reshapes each image into a 64 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(digits.data[0])\n",
    "print(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Now let's create a logistic regression model to classify digits in the MNIST dataset, fit the model to a training set, then test it on a testing set. We will also compute the classification rates and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: \n",
    "\n",
    "If you are running this in class on Binder, you may want to decrease `max_iter` to 100 to get it to run faster.  If you are running this on your machine, you can keep it at 10000 and it should run just fine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='lbfgs', multi_class = 'multinomial', max_iter=10000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(model.score(X_train,y_train))\n",
    "logistic_regression_score = model.score(X_test,y_test)\n",
    "print(logistic_regression_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predicted = model.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives a fancy plot of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "array = metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "df_cm = pd.DataFrame(array, range(10),\n",
    "                  range(10))\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Coefficients\n",
    "\n",
    "Once the classification model has been created, we have a list of coefficients for each label. Let's try to visualize what those coefficients are telling us about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(2,5,j+1)\n",
    "    plt.imshow(model.coef_[j].reshape(8,8), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Classification Models\n",
    "\n",
    "There are many classification models built into `scikit-learn`. We have studies some of these and not others, but the procedure for splitting into training/testing sets, training the models and testing performance is basically the same in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "modelSVM = SVC(kernel=\"linear\",C=1)\n",
    "# C is a regularization parameter. The default is C=1, so this is not necessary to include. It is here for demonstration.\n",
    "modelSVM.fit(X_train, y_train)\n",
    "\n",
    "SVM_score = modelSVM.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression Score: ',logistic_regression_score)\n",
    "print('Support Vector Machine Score: ',SVM_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Nearest Neighbors\n",
    "\n",
    "Another extremely simple supervised classification technique is called *$k$-Nearest Neighbors*. To determine the class of a vector $\\vec{x}$, we look at the $k$ nearest points in the training data with respect to, say, Euclidean distance. The most common label amongst those is neighbors is then assigned to $\\vec{x}$.\n",
    "\n",
    "Let's try it on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "kNN_score = knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression Score: ',logistic_regression_score)\n",
    "print('Support Vector Machine Score: ',SVM_score)\n",
    "print('5-Nearest Neighbors Score: ',kNN_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "- This simple method does even better than logistic regression or SVM! In general, comparative performance of the models will depend on the application (i.e., kNN does worse than LR on some data).\n",
    "\n",
    "- The fitting essentially takes no time, because all\n",
    "`fit` does is store all the training data.  This results in &ldquo;large&rdquo;\n",
    "models that require quite a bit memory.  \n",
    "\n",
    "- For large datasets, `predict` is slow\n",
    "if there are many points, because it is costly to search for nearby\n",
    "neighbors.\n",
    "\n",
    "- $k$-NN can also run into problems for high-dimensional data, due to the \"Curse of Dimensionality\" https://en.wikipedia.org/wiki/Curse_of_dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting thing about $k$-NN classification is that it doesn't actually require knowledge of the **vectors**, just the **distances** between them. This means that $k$-NN is well-defined in general metric spaces which may not be vector spaces. \n",
    "\n",
    "Some alternative metrics on $\\mathbb{R}^d$ are built in to `scikit-learn`. A list of them is available here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Experiment with $k$-NN classification of MNIST by changing the parameters. In particular:\n",
    "\n",
    "1) What happens if you change the number of neighbors? Make a plot of the classification rate over various values of $k$. \n",
    "\n",
    "2) Try some other metrics on $\\mathbb{R}^{64}$. Which metric does the *best* job? Which metric does the *worst* job?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network: Multi-Layer Perceptron\n",
    "\n",
    "There are lots of parameters to experiment with here. The following choice gives the best results that I found, but I didn't mess with it for too long. See the documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(20, 20), random_state=1)\n",
    "\n",
    "mlp.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_score = mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression Score: ',logistic_regression_score)\n",
    "print('Support Vector Machine Score: ',SVM_score)\n",
    "print('5-Nearest Neighbors Score: ',kNN_score)\n",
    "print('Multi-Layer Perceptron Score:', mlp_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "The `fashion-mnist` is another interesting dataset for image classification. It contains preprocessed pictures of clothing items from 10 classes together with labels. The code below reads in and displays part of the `fashion-mnist` dataset. Explore this dataset and find which classification algorithm gives the best performance on it.\n",
    "\n",
    "**Note:** You will have to figure out how to get the data into an appropriate form to train the models. \n",
    "\n",
    "**Note:** This dataset is a lot bigger than what we have been using. The training steps may take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "# Load the data.\n",
    "data = pd.read_csv('fashion-mnist-demo.csv')\n",
    "\n",
    "# Create the mapping between numeric category and category name.\n",
    "label_map = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot',\n",
    "}\n",
    "\n",
    "# Print the data table.\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choose a number of images to display. Should be a perfect square to make the display look good.\n",
    "numbers_to_display = 25\n",
    "\n",
    "# Calculate the number of cells that will hold all the images.\n",
    "grid_size = int(np.sqrt(numbers_to_display))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Go through the first images in a training set and plot them.\n",
    "for plot_index in range(numbers_to_display):\n",
    "    # Extrace image data.\n",
    "    digit = data[plot_index:plot_index + 1].values\n",
    "    digit_label = digit[0][0]\n",
    "    digit_pixels = digit[0][1:]\n",
    "\n",
    "    # Calculate image size (remember that each picture has square proportions).\n",
    "    image_size = int(np.sqrt(digit_pixels.shape[0]))\n",
    "    \n",
    "    # Convert image vector into the matrix of pixels.\n",
    "    frame = digit_pixels.reshape((image_size, image_size))\n",
    "    \n",
    "    # Plot the image matrix.\n",
    "    plt.subplot(grid_size, grid_size, plot_index + 1)\n",
    "    plt.imshow(frame, cmap='Greys')\n",
    "    plt.title(label_map[digit_label])\n",
    "    plt.axis('off')\n",
    "\n",
    "# Plot all subplots.\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Preprocess the data a bit until it is the correct form. Then train some classifiers and test their performance. What kind of results do you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "An important task in \"unsupervised learning\" is to *cluster* data. That is, we wish to find groupings of similar data without any knowledge of 'ground truth' labels. Let's explore some methods for doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Means Clustering\n",
    "\n",
    "Suppose that we have a point cloud of data $X = \\{\\vec{x}_1,\\ldots,\\vec{x}_N\\}$ with each $\\vec{x}_j \\in \\mathbb{R}^d$. Our goal is to divide $X$ into $k$ \"clusters\", where $k$ is some positive integer we choose ahead of time. This just means we want to partition $X$ into $k$ sets. Our partition should reflect natural groupings of the data.\n",
    "\n",
    "As usual, let's construct some toy data to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "K = 2 # classes\n",
    "N = 100 # in each class\n",
    "dimension = 2\n",
    "X, y = make_blobs(n_samples=N*K, centers=K, n_features=dimension, random_state=12)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see visually that our data roughly lies in two clusters. Moreover, we have 'ground truth' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of illustration, suppose we have no labels and that our data lives in a high dimension that we can't plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to *partition* $X$ into $\\{S_1,\\ldots,S_k\\}$ disjoint nonempty subsets $S_j \\subset X$. Let $\\mu_j$ denote the mean of the points in $S_j$; these are called *cluster centers* in this context. We want our partition to minimize the quantity\n",
    "$$\n",
    "\\sum_{j=1}^k \\sum_{\\vec{x} \\in S_j} \\|\\vec{x} - \\mu_j\\|^2.\n",
    "$$\n",
    "\n",
    "The idea is that the winning partition has the data clustered as tightly as possible around the $k$ means (hence the name of the algorithm). \n",
    "\n",
    "It is not possible to solve for this partition explicitly, so we will search for it iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write code to compute the $k$-means clustering partition. \n",
    "\n",
    "**Spoiler:** This function is built into `scikit-learn`. The point is to build the algorithm ourselves first to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $k$-Means Clustering Algorithm\n",
    "\n",
    "When writing our algorithm, we'll take the opportunity to demonstrate some more useful `numpy` tricks. These will be pointed out as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize with Random Cluster Centers\n",
    "\n",
    "A useful function for this task is `np.random.choice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "np.random.choice(10,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our `cluster_centers` function. If you are new to Python, note the syntax here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_centers(X,K):\n",
    "    return X[np.random.choice(len(X),size=K)]\n",
    "    # Pull out entries of X given by the random choice of K indices\n",
    "\n",
    "# Testing\n",
    "print(cluster_centers(X,2))\n",
    "print(cluster_centers(X,2))\n",
    "print(cluster_centers(X,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:  Determine Clusters\n",
    "\n",
    "For each point in $X$, we figure out which cluster center is nearest to it. \n",
    "\n",
    "We will employ a useful trick called *numpy broadcasting*. If we apply arithmetic operations to `numpy` arrays of incompatible sizes, numpy broadcasting will make sense of this by 'broadcasting' the smaller array over the larger one. This only works under certain conditions on the sizes, so we have to put some thought into setting it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arrays to test on\n",
    "A = np.array([[1,2]])\n",
    "B = np.array([[0,0],[1,1],[2,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't make sense mathematical to add these different-sized if we think of them as matrices. On the other hand, `numpy` interprets addition as: 'add the row of `A` to *each* row of `B`. \n",
    "\n",
    "These sort of \"broadcasting\" operations can be used to make your code very clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't make sense to add these arrays mathematically\n",
    "A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get even trickier by employing the `np.newaxis` function which takes a 1D array to a 2D array, a 2D array to a 3D array, etc. The way that the function affects the array depends on which 'slot' we use it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a new test array\n",
    "C = np.array([[1,2],[3,4]])\n",
    "Cnew = C[:,np.newaxis,:]\n",
    "print(Cnew.shape)\n",
    "Cnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives an error. To see the general rules for broadcasting, check here: https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "B+C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, our reshaped array `Cnew` follows the rules to be broadcast with `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print((B+Cnew).shape)\n",
    "B+Cnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our function. The input is the dataset `X` and a collection of cluster centers `centers` (e.g., the output of `cluster_centers(X,K)`). The output is an array indicating the index of the cluster center to which each element $\\vec{x}_j$ of $X$ belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(X, centers):\n",
    "    distances = np.linalg.norm(X - centers[:, np.newaxis,:], axis=2)\n",
    "    return np.argmin(distances, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "centers = cluster_centers(X,2)\n",
    "closest(X,centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Update Centers\n",
    "\n",
    "Now we define an 'update' function. The input is our dataset `X` and a set of cluster centers `centers`. The output is a new collection of cluster centers, obtained by\n",
    "- partitioning the data according to the input cluster centers,\n",
    "- computing the mean within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_centers(X, centers):\n",
    "    c = closest(X, centers)\n",
    "    K = len(np.unique(c)) # Determine K by finding the number of labels in c\n",
    "    return np.array([X[c==k].mean(axis=0) for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "new_centers(X,centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Iterate the procedure\n",
    "\n",
    "We can now write our algorithm. We simply iterate the procedure above until the cluster center updates stop moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeans(X, K, max_iter = 10000):\n",
    "    # Initializations\n",
    "    centers = X[np.random.choice(len(X),size=K)]\n",
    "    iteration = 0\n",
    "    Delta = 1\n",
    "    # While loop with a hard limit on number of iterations\n",
    "    while Delta > .001 and iteration < max_iter:\n",
    "        moved = new_centers(X,centers)\n",
    "        Delta = np.linalg.norm( moved - centers )\n",
    "        iteration = iteration+1\n",
    "        centers = moved\n",
    "    print('Iterations to converge: ', iteration)\n",
    "    labels = closest(X,centers)\n",
    "    # Output is a tuple\n",
    "    return centers, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, labels = kMeans(X,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "p1 = fig.add_subplot(1,2,1)\n",
    "p1.scatter(X[:,0],X[:,1],c=y)\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "p2 = fig.add_subplot(1,2,2)\n",
    "p2.scatter(X[:,0],X[:,1],c=1-labels) # Use 1-labels so the colors match up\n",
    "p2.scatter(centers[:,0],centers[:,1], marker = '^', c = 'r')\n",
    "plt.title('KMeans Algorithm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great! Of course, there is not reason that $k$-Means should perfectly replicate 'ground truth' labels if the data is not truly clustered. \n",
    "\n",
    "Some other issues:\n",
    "- This is a randomly-initialized iterative algorithm and there is no guarantee that we find an absolute minimum!\n",
    "- We knew that there should be 2 classes ahead of time. The $k$ in $k$-Means is chosen by the user, so it is definitely possible to make the 'wrong' choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, labels = kMeans(X,5)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=labels) \n",
    "plt.scatter(centers[:,0],centers[:,1], marker = '^', c = 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Try your $k$-Means algorithm on examples `X1` through `X4` below. For each example, try several values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=500, center_box=(-3,3), centers=3, random_state=6)\n",
    "\n",
    "\n",
    "X2, y2 = make_blobs(n_samples=1000, centers=4, random_state=1)\n",
    "\n",
    "xs = np.linspace(0,2*np.pi,500)\n",
    "X3 = np.array([np.cos(xs),np.sin(xs)]).T\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "X4, y4 = make_circles(n_samples=500, noise = 0.02, random_state = 3)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "p1 = fig.add_subplot(2,2,1)\n",
    "p1.scatter(X1[:,0],X1[:,1])\n",
    "plt.title('Example 1')\n",
    "\n",
    "p2 = fig.add_subplot(2,2,2)\n",
    "p2.scatter(X2[:,0],X2[:,1])\n",
    "plt.title('Example 2')\n",
    "\n",
    "p3 = fig.add_subplot(2,2,3)\n",
    "p3.scatter(X3[:,0],X3[:,1])\n",
    "plt.title('Example 3');\n",
    "\n",
    "p4 = fig.add_subplot(2,2,4)\n",
    "p4.scatter(X4[:,0],X4[:,1])\n",
    "plt.title('Example 4');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Means with SciKit-Learn\n",
    "\n",
    "As mentioned above, `scikit-learn` has $k$-Means Clustering capability, as well as several useful functions for analyzing the results.\n",
    "\n",
    "Let's first try the `scikit-learn` implementation on our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These agree with our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, labels = kMeans(X,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(centers)\n",
    "print(np.linalg.norm(labels - kmeans.labels_))\n",
    "# Might need to switch to 1-labels above to see the correct result, since we initialize randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Means on MNIST\n",
    "\n",
    "Of course we need to try out the algorithm on MNIST!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "MNIST, MNISTlabels = load_digits(return_X_y=True)\n",
    "# The included option automatically gives us the vectors and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let&rsquo;s cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the ground truth labels (namely `MNISTlabels`) are known, there are a variety of metrics that one can use to evaluate the quality of clustering. Let's use the \"Adjusted Rand Index\" (see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html). For this score, random labeling should give something close to 0, perfect labeling gives 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "adjusted_rand_score(MNISTlabels, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does a bad job. Of course, 3 clusters is not a good choice for MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run $k$-Means clustering on MNIST for a range of choices for $k$, compute adjusted rand scores for each choice, then plot the results. What looks like the best choice for $k$? Does that agree with intuition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for j in range(1,25):\n",
    "    kmeans = KMeans(n_clusters=j).fit(MNIST)\n",
    "    scores.append(adjusted_rand_score(MNISTlabels, kmeans.labels_))\n",
    "\n",
    "plt.plot(list(range(1,25)),scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look which digits are getting clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10).fit(MNIST)\n",
    "\n",
    "digit = 7 # Note: label 7 doesn't necessarily correspond to the digit 7. Label names are random!\n",
    "images = MNIST[kmeans.labels_ == digit]\n",
    "\n",
    "fig, axes = plt.subplots(10,10,figsize=(10,10))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        axes[i,j].axis('off')\n",
    "        axes[i,j].imshow(images[i * 10 + j].reshape(8,8),cmap='gray',interpolation='sinc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Many Clusters?\n",
    "\n",
    "Since we must choose a value of $k$ to run $k$-Means clustering, there is a big questions when doing *unsupervised learning* (i.e., we don't have 'ground truth' labels): what is the correct choice of $k$?\n",
    "\n",
    "There is no *real* answer, but we can make an educated guess by looking at the *inertia* of the clustering. This is just the value of the function we were optimizing to begin with, evaulated on our clustering partition:\n",
    "$$\n",
    "\\sum_{j=1}^k \\sum_{\\vec{x} \\in S_j} \\|\\vec{x} - \\mu_j\\|^2.\n",
    "$$\n",
    "\n",
    "This is computed in `scikit-learn` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(X)\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let&rsquo;s plot this within-cluster sum-of-squares for the clusters computed via `KMeans` for multiple choices of `n_clusters`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,10)), [KMeans(n_clusters=j).fit(X).inertia_ for j in range(1,10)] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the number of clusters, the inertia will *always* decrease, so we are not looking for a local min. Instead, we look for an \"elbow\", where the slope of the intertia curve abruptly changes. If such an elbow appears, this is generally accepted to be an optimal number of clusters. For the dataset `X`, this tells us that the optimal number of clusters is 2, which should agree with our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Run this \"elbow analysis\" on examples `X1` through `X4` from above. Can you determine an optimal number of clusters in each case? What if you run the same sort of analysis on the MNIST dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
